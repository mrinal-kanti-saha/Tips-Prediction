---
title: "Waiter's Tip Prediction in R"
author: "Mrinal Kanti Saha (21234) and Parth Maheshwari (21235)"
date: "May 5, 2022"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading all the required libraries

```{r lib, results = FALSE, message = FALSE}
library(tidyverse)
library(naniar)
library(GGally)
library(caret)
library(car)
library(dplyr)
library(reshape)
library(leaps)
library(MASS)
library(corrplot)
library(ggalt)
```

# Loading the dataset

```{r dataset}
# set working directory
setwd("/home/msc1/R_Project/Dataset_Tips")
# check the directory
getwd()
# import the dataset and read the strings as factors
tips <- read.csv("./tips.csv", stringsAsFactors = TRUE)
```

# Exploratory Data Analysis

```{r eda, warning = FALSE}
# Dimension of the data, no of rows and columns
dim(tips)
# First 6 rows
head(tips)
# Names of the columns
names(tips)
# Structure of the dataframe tips
str(tips)
# 6 point info about each feature
summary(tips)

# Do we have missing values ?
# Plot of missing values
vis_miss(tips)
```

All the data values are present.

```{r numdf}
# Data frame of numerical features
num_cols <- unlist(lapply(tips, is.numeric))
tips_num <- tips[, num_cols]
head(tips_num)
```

# Data Visualization

```{r viz}
ggplot(as.data.frame(tips), aes(factor(sex), total_bill, fill = sex)) +     
  geom_col(position = 'dodge')

```

Inference: There isn't much difference when it comes to pay for total_bill. Males leading the table

```{r viz0}
ggplot(tips, aes(x = total_bill)) +
  geom_histogram(fill = "#F78e46", colour = "black") +
  facet_grid(smoker ~ .)
```

Inference: Non-smoker are  higher than smoker in range of 10-30 range of total_bill

```{r viz1}
ggplot(tips, aes(x = total_bill, fill = time)) +
  geom_histogram(position = "identity", alpha = 0.4)

```

Inference: Dinner is favored than lunch for any number of total_bill

```{r viz2}

g <- ggplot(tips, aes(time))
g + geom_bar(aes(fill=day), width = 0.5) + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
  labs(title="Sizewise Bar Chart", 
       subtitle="Number of People on days", 
       caption="Source: Size from 'tips' dataset")

```

Inference: People prefer dinner over lunch in large number

```{r viz3}
g <- ggplot(tips, aes(day, size))
g + geom_bar(stat="identity", width = 0.5, fill="red") + 
  labs(title="Bar Chart", 
       subtitle="DAY vs SIZE", 
       caption="Source: TIPS DATASET") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

Inference: The size of the party is much higher on the weekends

```{r viz4}
pie <- ggplot(tips, aes(x = "", fill = factor(day))) + 
  geom_bar(width = 1) +
  theme(axis.line = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  labs(fill="class", 
       x=NULL, 
       y=NULL, 
       title="Pie Chart of Day", 
       caption="Source: tips")

pie + coord_polar(theta = "y", start=0)
```

Inference: People are most likely to have food on saturday and sunday

```{r viz5}
# Pairplot
ggpairs(data = tips_num, title = "Pairplots of numerical features",
        upper = list(continuous = wrap("cor", size = 7)))
```

Inference: The data seems linear between tip and total_bill

# Feature Engineering

```{r featureengg}
tips %>%
  group_by(day) %>%
  summarise_at(vars(tip), list(name = mean))
# 1 Fri    2.73 1
# 2 Sat    2.99 3
# 3 Sun    3.26 4
# 4 Thur   2.77 2
# From this, we are trying to derive an ordering for the day variable.
tips$day <- as.factor(ifelse(tips$day=="Thur", 2,
                      ifelse(tips$day=="Fri", 1,
                      ifelse(tips$day=="Sat", 3,
                      ifelse(tips$day=="Sun", 4, 5)))))

# Convert categorical features using One Hot Encoding
# dummy <- dummyVars(" ~ .", data = tips)
# new_tips <- data.frame(predict(dummy, newdata = tips))
# head(new_tips)
# head(tips)

# Convert categorical features using Ordinal Encoding
new_tips <- data.frame(tips)
new_tips$day <- unclass(tips$day)
new_tips$time <- unclass(tips$time)
new_tips$sex <- unclass(tips$sex)
new_tips$smoker <- unclass(tips$smoker)
head(new_tips)
str(new_tips)
```

# Building a Generalised Model

```{r glm}
model <- lm(tip ~ ., data = new_tips)
summary(model)
```

## Checking of linear relationship between study and explanatory variables

```{r lr}
# Correlation check
corr <- cor(tips_num)
corrplot(corr, method = "color")
```

Observations :
tip has high correlation with total_bill and then with size. and negligible correlation with others.
total_bill has high correlation with size and negligible correlation with others.
day has high correlation with time and negligible correlation with others.

## Checking for Influence Points

```{r inflm1}
inflm.model <-  influence.measures(model)
which(apply(inflm.model$is.inf, 1, any))
# Which observations 'are' influential
sum_inflm <- summary(inflm.model)

n = dim(sum_inflm)[1]
n
k = dim(sum_inflm)[2]
k

# Limits of DFBETAS and DFFITS
DFbetalim = 2 / sqrt(n)
DFbetalim
DFfitslim = 2 * sqrt(k/n)
DFfitslim
list_inflm = list()
```

A point is termed as an influence point and needs further observation,   
if COOK's D-statistic(D) > 1  
if DFBETAS > 2/sqrt(n) = 2/sqrt(20)) = 0.4364358, i.e DFBETAS > 0.4364358 
if DFFITS > 2 * sqrt(k/n)  = 2*sqrt(11/20) = 1.447494, i.e DFFITS > 1.447494  

```{r inflm2}
for (i in 1 : n) {
  # Checking Cook's D-statistic
  if (sum_inflm[i, "cook.d"] > 1) {
    append(list_inflm, c(i))
  }
  # Checking DFBETAS
  for (j in 1 : (k - 4)) {
    if (sum_inflm[i, j] > DFbetalim) {
      append(list_inflm, c(i))
    }
  }
  # Checking DFFITS
  if (sum_inflm[i, "dffit"] > DFfitslim) {
    append(list_inflm, c(i))
  }
}

# Get the list of influential points
list_inflm <- unique(list_inflm)
print(length(list_inflm))
```

Observation : None of the points need to be removed.

## Multicolinearity Check (for numeric only)

```{r multicolinearity}
model_num <- lm(tip ~ ., data = tips_num)
summary(model_num)
# Variation Inflation Factor
vif(model_num)
```

Observation : No such multicolinearity as VIF < 4 for all variables

## Check the normality of the residuals

```{r norm1}
# Calculate standardised residuals
res <- resid(model)
mean(res)
```

The assumption of the zero mean holds. Mean of residuals = -5.688101e-17 ~ 0

```{r norm2}
plot(fitted(model), res)
# Add a horizontal line at 0
abline(0, 0)
```

The residuals vs the fitted values seems to form a funnel, failing the assumption of Homoskadasticity

```{r norm3}
df_res <- as.data.frame(res)
```

### Distribution Curves

```{r dist}
ggplot(data = df_res, aes(x = res)) +
  geom_histogram(bins = 25, aes(y =..density..), fill = "orange") +
  geom_density()
```

The residuals can be said to be approximated to normal distribution, but still, we will try doing other tests and proceed with necessary measures.

### QQ-Plot

```{r qq}
qqPlot(df_res$res)
```

The data doesn't seems to fit into a straight line. Hence, the residuals cannot be assumed to be normal.

### Shapiro-Wilks Test

```{r swt}
# It is based on the correlation between the data and the corresponding normal scores.
# H0 : Part of Normal distribution
# H1 : Not part of normal distribution
shapiro.test(df_res$res)
```

The p-value of the SWT is less than 0.05, therefore, we reject it as a normal distribution.

From Shapiro-Wilk test, we get that the distribution of the residuals are not normal.  
We should be checking the existence of outliers in both dependent and independent variables now.  
But the model without outliers, is poorer than the normal, and therefore this code is given below commented.

```{r}
# # Outlier Detection
# meltData <- melt(tips_num)
# p <- ggplot(meltData, aes(factor(variable), value))
# p + geom_boxplot() + facet_wrap(~variable, scale="free")
# 
# # Removing outliers by using IQR
# # tip
# Q1 <- quantile(new_tips$tip, .25)
# Q3 <- quantile(new_tips$tip, .75)
# IQR <- IQR(new_tips$tip)
# new_tips <- subset(new_tips, new_tips$tip > (Q1 - 1.5*IQR) & new_tips$tip < (Q3 + 1.5*IQR))
# dim(new_tips)
# # total_bill
# Q1 <- quantile(new_tips$total_bill, .25)
# Q3 <- quantile(new_tips$total_bill, .75)
# IQR <- IQR(new_tips$total_bill)
# new_tips <- subset(new_tips, new_tips$total_bill > (Q1 - 1.5*IQR) & new_tips$total_bill < (Q3 + 1.5*IQR))
# dim(new_tips)
# # size
# Q1 <- quantile(new_tips$size, .25)
# Q3 <- quantile(new_tips$size, .75)
# IQR <- IQR(new_tips$size)
# new_tips <- subset(new_tips, new_tips$size > (Q1 - 1.5*IQR) & new_tips$size < (Q3 + 1.5*IQR))
# dim(new_tips)
# # By this technique, we have managed to reduce the observations from 244 to 221.
# 
# # Building a Generalised Model
# model2 <- lm(tip ~ ., data = new_tips)
# summary(model2)
# 
# ors <- rstandard(model2)
# df_res <- as.data.frame(ors)

# Now, we would have to plot the distribution curve and the QQ plot to verify
# if our data has become Normal.
# ggplot(data = df_res, aes(ors)) +
#   geom_histogram(bins= 25, aes(y =..density..), fill = "orange") +
#   geom_density()
# # The residuals can be said to be approximated to normal distribution, 
# # but we will confirm it with QQ-plot and SWT.
# 
# # QQ-Plot
# qqPlot(df_res$ors)
# # The data seems to fit into a straight line still. Hence, further treatment is required.
# 
# # Shapiro-Wilks Test
# shapiro.test(df_res$ors)
# # W = 0.98182, p-value = 0.006065
# # The p-value of the SWT is less than 0.05, therefore, we reject it, yet again, as a normal distribution.
```

### Box Cox Transformation

```{r boxcox}
bc <- boxcox(model)
(lambda <- bc$x[which.max(bc$y)])
new_model <- lm((tip^lambda - 1)/lambda ~ ., data = new_tips)
summary(new_model)

res <- resid(new_model)
mean(res)
```

The assumption of the zero mean holds. Mean of residuals = 3.323702e-18 ~ 0

```{r assump}
plot(fitted(new_model), res)
# Add a horizontal line at 0
abline(0, 0)
abline(1, 0)
abline(-1, 0)
```

The residuals vs the fitted values seems to form a horizontal band around 0, validating the assumption of Homoskadasticity.

### Reaffirming the normality of the residuals after boxcox transformation

```{r renorm1}
df_res <- as.data.frame(res)

# Distribution Curves
ggplot(data = df_res, aes(x = res)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()
```

The residuals can be said to be approximated to normal distribution, but still, we will try doing other tests and proceed with necessary measures.

```{r renorm2}
# Shapiro-Wilks Test
shapiro.test(df_res$res)
```

The residuals are normally distributed as p value greater than 0.05

```{r renorm3}
# QQ-Plot
qqPlot(df_res$res)
```

The data seems to fit into a straight line. Hence, the residuals can be assumed to be normal.  
There are two observations that seems to be the outliers and hence we would remove those.

```{r ytransform}
dim(new_tips)
new_tips <- new_tips[-c(173, 238), ]
dim(new_tips)

# Transforming Y variable using boxcox
for(i in 1 : dim(new_tips)[1])
  new_tips[i, "tip"] = ((new_tips[i, "tip"]^lambda - 1) / lambda)
head(new_tips)
```

# Feature Selection

```{r fs}
# Forward Selection
FSR = regsubsets(tip ~ ., data = new_tips, method = "forward")
summary(FSR)
Modelsummary.1 = cbind(summary(FSR)$which, R2=summary(FSR)$rsq, SSres=summary(FSR)$rss, 
                     AdjR2=summary(FSR)$adjr2, Cp=summary(FSR)$cp, BIC=summary(FSR)$bic)
Modelsummary.1

# Backward elimination
BER = regsubsets(tip ~ ., data = new_tips, method = "backward")
Modelsummary.2 = cbind(summary(BER)$which, R2=summary(BER)$rsq, SSres=summary(BER)$rss,
                       AdjR2=summary(BER)$adjr2, Cp=summary(BER)$cp, BIC=summary(BER)$bic)
Modelsummary.2

# Stepwise Regression
SWR = regsubsets(tip ~ ., data = new_tips, method = "seqrep")
Modelsummary.3 = cbind(summary(SWR)$which, R2=summary(SWR)$rsq, SSres=summary(SWR)$rss,
                       AdjR2=summary(SWR)$adjr2, Cp=summary(SWR)$cp, BIC=summary(SWR)$bic)
Modelsummary.3
```

Recommendation : total_bill, size from all the 3 methods

# Final Model 
## Training the mode;

```{r traintest}
set.seed(108)

# Splitting the dataset into train and test 8:2
trainPartitionRows <- createDataPartition(new_tips$tip, p = .80, list = FALSE)
nrow(new_tips)
nrow(trainPartitionRows)
head(trainPartitionRows)
trainDataset <- new_tips[ trainPartitionRows, ]
testDataset  <- new_tips[-trainPartitionRows, ]
head(trainDataset)
head(testDataset)

# Repeated Cross Validation
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
tips_model <- train(tip ~ total_bill + size,
                    data = new_tips,
                    method = "lm",
                    trControl = ctrl,
)
tips_model
```

Train RMSE = 0.3598576  
R-squared = 0.4966135  
MAE = 0.2927397

```{r summary}
summary(tips_model)
```

## Saving model to an RDS file

```{r rds}
# Save a single object to a file
saveRDS(tips_model, "./Model_tips.rds")
```

## Running the model on test data

```{r rmse}
str(testDataset)
test <- testDataset %>% dplyr::select(-c(tip))
pred <- predict(tips_model, test)

vals <- data.frame(predicted = pred, actual = testDataset$tip)

test_RMSE <- sqrt(mean((vals$predicted - vals$actual)^2))
test_RMSE
```

Test RMSE = 0.3297749  
Test RMSE 0.3297749 < Train RMSE 0.3598576, therefore the model has genralised well.

# Remarks  

1. The model has pretty low R-squared but the Test RMSE is still less than the Train RMSE.  
2. Using One-Hot Encoding for the categorical variables yielded even less R-squared value, hence, the choice of Ordinal Encoding has been taken up, even though there was no such ordering possible among the categories.  
3. Outliers were removed by using the IQR tactics, but that too yielded a much poorer model.  

# End of Project